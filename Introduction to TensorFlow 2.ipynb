{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1\n",
    "\n",
    "Build a simple model in TensorFlow. \n",
    "\n",
    " Build and train neural network models using TensorFlow 2.x\n",
    "You need to understand the foundational principles of machine learning (ML) and deep learning (DL)\n",
    "using TensorFlow 2.x. You need to know how to:\n",
    "* Use TensorFlow 2.x.\n",
    "* Build, compile and train machine learning (ML) models using TensorFlow.\n",
    "* Preprocess data to get it ready for use in a model.\n",
    "* Use models to predict results.\n",
    "* Build sequential models with multiple layers.\n",
    "* Build and train models for binary classification.\n",
    "* Build and train models for multi-class categorization.\n",
    "* Plot loss and accuracy of a trained model.\n",
    "* Identify strategies to prevent overfitting, including augmentation and dropout.\n",
    "* Use pretrained models (transfer learning).\n",
    "* Extract features from pre-trained models.\n",
    "* Ensure that that inputs to a model are in the correct shape.\n",
    "* Ensure that you can match test data to the input shape of a neural network.\n",
    "* Ensure you can match output data of a neural network to specified input shape for test data.\n",
    "* Understand batch loading of data.\n",
    "* Use callbacks to trigger the end of training cycles.\n",
    "* Use datasets from different sources.\n",
    "* Use datasets in different formats, including json and csv.\n",
    "* Use datasets from tf.data.datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Flatten\n",
    "# tf.enable_eager_execution()\n",
    "# tf.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test), info = tfds.load('fashion_mnist', split=['train', 'test'], shuffle_files=True, as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_img(image, label):\n",
    "    \"\"\"Normalizes images; uint8 -> float32\"\"\"\n",
    "    \n",
    "    return tf.cast(image,tf.float32) / 255., label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline\n",
    "\n",
    "### Why do we need to set up a good training pipeline?\n",
    "\n",
    "When working with GPUs and TPUs, we can radically reduce the time required to execute a single training step. Achieving peak performance requires an efficient input pipeline that delivers data for the next step before the current step has finished. The `tf.data` API helps to build flexible and efficient input pipelines. \n",
    "\n",
    "### Steps taken to set up training pipeline\n",
    "\n",
    "* We use a map function with the `normalize_img` function we created. We let tf autoset the number of parallel calls to make this happen as fast as possible\n",
    "* Because the entire training dataset can fit in memory, we can cache before shuffling to get better performance. \n",
    "* For true randomness, we set the shuffle buffer to the full dataset size (which can be found in `info.splits['train'].num_examples`\n",
    "* By batching after shuffling, we get unique batches for each epoch.\n",
    "* Finally, we end the pipeline by prefetching for performances.\n",
    "\n",
    "**_NOTE:_** The prefetch transformation provides benefits any time there is an opportunity to overlap the work of a \"producer\" with the work of a \"consumer\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training pipeline\n",
    "\n",
    "train = train.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train = train.cache()\n",
    "train.shuffle(info.splits['train'].num_examples)\n",
    "train = train.batch(128)\n",
    "train.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Pipeline\n",
    "\n",
    "A bit different from training pipeline. \n",
    "\n",
    "* We still normalize the testing dataset just like the training dataset. This is important!\n",
    "* For testing, we batch our data before caching. This way\n",
    "* We still end with a `prefetch()` call to optimize our pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test = test.batch(128)\n",
    "test = test.cache()\n",
    "test = test.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28, 28, 1)))\n",
    "model.add(Dense(128, activation='tanh'))\n",
    "model.add(Dense(64, activation='tanh'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 12s 25ms/step - loss: 1.0317 - accuracy: 0.6834 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 10s 22ms/step - loss: 0.6566 - accuracy: 0.7857 - val_loss: 0.6225 - val_accuracy: 0.7857\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 10s 21ms/step - loss: 0.5685 - accuracy: 0.8090 - val_loss: 0.5651 - val_accuracy: 0.8025\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 10s 21ms/step - loss: 0.5227 - accuracy: 0.8208 - val_loss: 0.5322 - val_accuracy: 0.8127\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 10s 22ms/step - loss: 0.4941 - accuracy: 0.8291 - val_loss: 0.5102 - val_accuracy: 0.8188\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 10s 21ms/step - loss: 0.4741 - accuracy: 0.8343 - val_loss: 0.4945 - val_accuracy: 0.8245\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 10s 21ms/step - loss: 0.4589 - accuracy: 0.8394 - val_loss: 0.4824 - val_accuracy: 0.8282\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 10s 21ms/step - loss: 0.4469 - accuracy: 0.8432 - val_loss: 0.4729 - val_accuracy: 0.8299\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 10s 22ms/step - loss: 0.4369 - accuracy: 0.8464 - val_loss: 0.4647 - val_accuracy: 0.8328\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 10s 21ms/step - loss: 0.4283 - accuracy: 0.8491 - val_loss: 0.4575 - val_accuracy: 0.8353\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23448504b48>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train, epochs=10, validation_data=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
